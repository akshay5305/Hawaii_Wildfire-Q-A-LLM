{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020618556701030927,
      "grad_norm": 6.751557350158691,
      "learning_rate": 9.938144329896908e-05,
      "loss": 4.0189,
      "step": 10
    },
    {
      "epoch": 0.041237113402061855,
      "grad_norm": 8.175172805786133,
      "learning_rate": 9.869415807560138e-05,
      "loss": 3.3328,
      "step": 20
    },
    {
      "epoch": 0.061855670103092786,
      "grad_norm": 13.8007230758667,
      "learning_rate": 9.800687285223368e-05,
      "loss": 2.8821,
      "step": 30
    },
    {
      "epoch": 0.08247422680412371,
      "grad_norm": 8.171640396118164,
      "learning_rate": 9.731958762886598e-05,
      "loss": 2.9231,
      "step": 40
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 5.173559665679932,
      "learning_rate": 9.663230240549829e-05,
      "loss": 2.8468,
      "step": 50
    },
    {
      "epoch": 0.12371134020618557,
      "grad_norm": 10.099005699157715,
      "learning_rate": 9.594501718213059e-05,
      "loss": 2.9039,
      "step": 60
    },
    {
      "epoch": 0.14432989690721648,
      "grad_norm": 6.754998683929443,
      "learning_rate": 9.525773195876289e-05,
      "loss": 2.896,
      "step": 70
    },
    {
      "epoch": 0.16494845360824742,
      "grad_norm": 5.00461483001709,
      "learning_rate": 9.457044673539519e-05,
      "loss": 2.4139,
      "step": 80
    },
    {
      "epoch": 0.18556701030927836,
      "grad_norm": 5.596841812133789,
      "learning_rate": 9.388316151202749e-05,
      "loss": 2.6944,
      "step": 90
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 6.639853477478027,
      "learning_rate": 9.31958762886598e-05,
      "loss": 2.4574,
      "step": 100
    },
    {
      "epoch": 0.2268041237113402,
      "grad_norm": 5.0004496574401855,
      "learning_rate": 9.25085910652921e-05,
      "loss": 2.8185,
      "step": 110
    },
    {
      "epoch": 0.24742268041237114,
      "grad_norm": 5.9237189292907715,
      "learning_rate": 9.18213058419244e-05,
      "loss": 2.679,
      "step": 120
    },
    {
      "epoch": 0.26804123711340205,
      "grad_norm": 5.975997447967529,
      "learning_rate": 9.11340206185567e-05,
      "loss": 2.7534,
      "step": 130
    },
    {
      "epoch": 0.28865979381443296,
      "grad_norm": 7.2662529945373535,
      "learning_rate": 9.0446735395189e-05,
      "loss": 2.64,
      "step": 140
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 6.187552452087402,
      "learning_rate": 8.975945017182131e-05,
      "loss": 2.5835,
      "step": 150
    },
    {
      "epoch": 0.32989690721649484,
      "grad_norm": 5.43507719039917,
      "learning_rate": 8.907216494845362e-05,
      "loss": 2.3708,
      "step": 160
    },
    {
      "epoch": 0.35051546391752575,
      "grad_norm": 4.362546443939209,
      "learning_rate": 8.838487972508591e-05,
      "loss": 2.5438,
      "step": 170
    },
    {
      "epoch": 0.3711340206185567,
      "grad_norm": 4.398411273956299,
      "learning_rate": 8.769759450171821e-05,
      "loss": 2.5507,
      "step": 180
    },
    {
      "epoch": 0.3917525773195876,
      "grad_norm": 6.081744194030762,
      "learning_rate": 8.701030927835052e-05,
      "loss": 2.2762,
      "step": 190
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 5.881648540496826,
      "learning_rate": 8.632302405498283e-05,
      "loss": 2.5367,
      "step": 200
    },
    {
      "epoch": 0.4329896907216495,
      "grad_norm": 4.951355934143066,
      "learning_rate": 8.563573883161513e-05,
      "loss": 2.5161,
      "step": 210
    },
    {
      "epoch": 0.4536082474226804,
      "grad_norm": 4.314056873321533,
      "learning_rate": 8.494845360824743e-05,
      "loss": 2.5081,
      "step": 220
    },
    {
      "epoch": 0.4742268041237113,
      "grad_norm": 4.431379795074463,
      "learning_rate": 8.426116838487973e-05,
      "loss": 2.5367,
      "step": 230
    },
    {
      "epoch": 0.4948453608247423,
      "grad_norm": 5.175920486450195,
      "learning_rate": 8.357388316151203e-05,
      "loss": 2.5807,
      "step": 240
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 6.56575870513916,
      "learning_rate": 8.288659793814434e-05,
      "loss": 2.2484,
      "step": 250
    },
    {
      "epoch": 0.5360824742268041,
      "grad_norm": 6.0302605628967285,
      "learning_rate": 8.219931271477663e-05,
      "loss": 2.5881,
      "step": 260
    },
    {
      "epoch": 0.5567010309278351,
      "grad_norm": 5.2963948249816895,
      "learning_rate": 8.151202749140893e-05,
      "loss": 2.3749,
      "step": 270
    },
    {
      "epoch": 0.5773195876288659,
      "grad_norm": 4.64434289932251,
      "learning_rate": 8.082474226804125e-05,
      "loss": 2.4326,
      "step": 280
    },
    {
      "epoch": 0.5979381443298969,
      "grad_norm": 5.518579959869385,
      "learning_rate": 8.013745704467355e-05,
      "loss": 2.372,
      "step": 290
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 3.749814748764038,
      "learning_rate": 7.945017182130585e-05,
      "loss": 2.402,
      "step": 300
    },
    {
      "epoch": 0.6391752577319587,
      "grad_norm": 3.6457698345184326,
      "learning_rate": 7.876288659793815e-05,
      "loss": 2.2538,
      "step": 310
    },
    {
      "epoch": 0.6597938144329897,
      "grad_norm": 5.207114219665527,
      "learning_rate": 7.807560137457045e-05,
      "loss": 2.5134,
      "step": 320
    },
    {
      "epoch": 0.6804123711340206,
      "grad_norm": 6.082962512969971,
      "learning_rate": 7.738831615120275e-05,
      "loss": 2.4929,
      "step": 330
    },
    {
      "epoch": 0.7010309278350515,
      "grad_norm": 5.324494361877441,
      "learning_rate": 7.670103092783506e-05,
      "loss": 2.3615,
      "step": 340
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 6.346109867095947,
      "learning_rate": 7.601374570446735e-05,
      "loss": 2.5137,
      "step": 350
    },
    {
      "epoch": 0.7422680412371134,
      "grad_norm": 4.682963848114014,
      "learning_rate": 7.532646048109965e-05,
      "loss": 2.3406,
      "step": 360
    },
    {
      "epoch": 0.7628865979381443,
      "grad_norm": 6.751570701599121,
      "learning_rate": 7.463917525773197e-05,
      "loss": 2.6347,
      "step": 370
    },
    {
      "epoch": 0.7835051546391752,
      "grad_norm": 6.632621765136719,
      "learning_rate": 7.395189003436427e-05,
      "loss": 2.4479,
      "step": 380
    },
    {
      "epoch": 0.8041237113402062,
      "grad_norm": 5.316795349121094,
      "learning_rate": 7.326460481099657e-05,
      "loss": 2.4615,
      "step": 390
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 5.599184513092041,
      "learning_rate": 7.257731958762887e-05,
      "loss": 2.4278,
      "step": 400
    },
    {
      "epoch": 0.845360824742268,
      "grad_norm": 3.965745210647583,
      "learning_rate": 7.189003436426117e-05,
      "loss": 2.2475,
      "step": 410
    },
    {
      "epoch": 0.865979381443299,
      "grad_norm": 6.588029861450195,
      "learning_rate": 7.120274914089347e-05,
      "loss": 2.5058,
      "step": 420
    },
    {
      "epoch": 0.8865979381443299,
      "grad_norm": 5.160254001617432,
      "learning_rate": 7.051546391752579e-05,
      "loss": 2.4979,
      "step": 430
    },
    {
      "epoch": 0.9072164948453608,
      "grad_norm": 3.8330681324005127,
      "learning_rate": 6.982817869415807e-05,
      "loss": 2.2574,
      "step": 440
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 4.326326370239258,
      "learning_rate": 6.914089347079037e-05,
      "loss": 2.2982,
      "step": 450
    },
    {
      "epoch": 0.9484536082474226,
      "grad_norm": 5.028839588165283,
      "learning_rate": 6.845360824742269e-05,
      "loss": 2.4893,
      "step": 460
    },
    {
      "epoch": 0.9690721649484536,
      "grad_norm": 4.526697158813477,
      "learning_rate": 6.776632302405499e-05,
      "loss": 2.6755,
      "step": 470
    },
    {
      "epoch": 0.9896907216494846,
      "grad_norm": 5.48358154296875,
      "learning_rate": 6.707903780068729e-05,
      "loss": 2.2855,
      "step": 480
    },
    {
      "epoch": 1.0103092783505154,
      "grad_norm": 3.7548716068267822,
      "learning_rate": 6.639175257731959e-05,
      "loss": 2.0842,
      "step": 490
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 3.990459442138672,
      "learning_rate": 6.570446735395189e-05,
      "loss": 1.8235,
      "step": 500
    },
    {
      "epoch": 1.0515463917525774,
      "grad_norm": 5.179961681365967,
      "learning_rate": 6.501718213058419e-05,
      "loss": 1.9325,
      "step": 510
    },
    {
      "epoch": 1.0721649484536082,
      "grad_norm": 5.075181007385254,
      "learning_rate": 6.43298969072165e-05,
      "loss": 1.8399,
      "step": 520
    },
    {
      "epoch": 1.0927835051546393,
      "grad_norm": 6.901611804962158,
      "learning_rate": 6.36426116838488e-05,
      "loss": 1.9788,
      "step": 530
    },
    {
      "epoch": 1.1134020618556701,
      "grad_norm": 5.6674065589904785,
      "learning_rate": 6.29553264604811e-05,
      "loss": 1.8704,
      "step": 540
    },
    {
      "epoch": 1.134020618556701,
      "grad_norm": 6.342062950134277,
      "learning_rate": 6.226804123711341e-05,
      "loss": 1.7972,
      "step": 550
    },
    {
      "epoch": 1.1546391752577319,
      "grad_norm": 5.774727821350098,
      "learning_rate": 6.158075601374571e-05,
      "loss": 1.8945,
      "step": 560
    },
    {
      "epoch": 1.175257731958763,
      "grad_norm": 7.532911777496338,
      "learning_rate": 6.0893470790378004e-05,
      "loss": 1.8547,
      "step": 570
    },
    {
      "epoch": 1.1958762886597938,
      "grad_norm": 8.414109230041504,
      "learning_rate": 6.020618556701031e-05,
      "loss": 1.9711,
      "step": 580
    },
    {
      "epoch": 1.2164948453608249,
      "grad_norm": 6.107491493225098,
      "learning_rate": 5.951890034364261e-05,
      "loss": 1.8776,
      "step": 590
    },
    {
      "epoch": 1.2371134020618557,
      "grad_norm": 6.9568891525268555,
      "learning_rate": 5.883161512027492e-05,
      "loss": 1.7654,
      "step": 600
    },
    {
      "epoch": 1.2577319587628866,
      "grad_norm": 10.106907844543457,
      "learning_rate": 5.814432989690722e-05,
      "loss": 2.1106,
      "step": 610
    },
    {
      "epoch": 1.2783505154639174,
      "grad_norm": 8.163461685180664,
      "learning_rate": 5.7457044673539515e-05,
      "loss": 1.8983,
      "step": 620
    },
    {
      "epoch": 1.2989690721649485,
      "grad_norm": 5.2958221435546875,
      "learning_rate": 5.676975945017182e-05,
      "loss": 1.9446,
      "step": 630
    },
    {
      "epoch": 1.3195876288659794,
      "grad_norm": 9.138164520263672,
      "learning_rate": 5.608247422680413e-05,
      "loss": 1.9165,
      "step": 640
    },
    {
      "epoch": 1.3402061855670104,
      "grad_norm": 8.050178527832031,
      "learning_rate": 5.539518900343643e-05,
      "loss": 1.9005,
      "step": 650
    },
    {
      "epoch": 1.3608247422680413,
      "grad_norm": 6.662418365478516,
      "learning_rate": 5.470790378006874e-05,
      "loss": 1.7968,
      "step": 660
    },
    {
      "epoch": 1.3814432989690721,
      "grad_norm": 6.05918550491333,
      "learning_rate": 5.402061855670103e-05,
      "loss": 1.9129,
      "step": 670
    },
    {
      "epoch": 1.402061855670103,
      "grad_norm": 10.079718589782715,
      "learning_rate": 5.333333333333333e-05,
      "loss": 1.6978,
      "step": 680
    },
    {
      "epoch": 1.422680412371134,
      "grad_norm": 5.8196940422058105,
      "learning_rate": 5.264604810996564e-05,
      "loss": 1.7087,
      "step": 690
    },
    {
      "epoch": 1.443298969072165,
      "grad_norm": 5.983964443206787,
      "learning_rate": 5.195876288659794e-05,
      "loss": 1.9178,
      "step": 700
    },
    {
      "epoch": 1.463917525773196,
      "grad_norm": 6.43078088760376,
      "learning_rate": 5.1271477663230236e-05,
      "loss": 1.8927,
      "step": 710
    },
    {
      "epoch": 1.4845360824742269,
      "grad_norm": 4.40748405456543,
      "learning_rate": 5.058419243986254e-05,
      "loss": 1.8583,
      "step": 720
    },
    {
      "epoch": 1.5051546391752577,
      "grad_norm": 9.463812828063965,
      "learning_rate": 4.989690721649485e-05,
      "loss": 1.9068,
      "step": 730
    },
    {
      "epoch": 1.5257731958762886,
      "grad_norm": 7.906622409820557,
      "learning_rate": 4.9209621993127145e-05,
      "loss": 1.8308,
      "step": 740
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 7.468343734741211,
      "learning_rate": 4.852233676975945e-05,
      "loss": 1.7813,
      "step": 750
    },
    {
      "epoch": 1.5670103092783505,
      "grad_norm": 7.109623908996582,
      "learning_rate": 4.783505154639176e-05,
      "loss": 1.874,
      "step": 760
    },
    {
      "epoch": 1.5876288659793816,
      "grad_norm": 6.77941370010376,
      "learning_rate": 4.7147766323024054e-05,
      "loss": 1.6864,
      "step": 770
    },
    {
      "epoch": 1.6082474226804124,
      "grad_norm": 7.742593288421631,
      "learning_rate": 4.646048109965636e-05,
      "loss": 1.7255,
      "step": 780
    },
    {
      "epoch": 1.6288659793814433,
      "grad_norm": 9.76014518737793,
      "learning_rate": 4.577319587628866e-05,
      "loss": 1.8859,
      "step": 790
    },
    {
      "epoch": 1.6494845360824741,
      "grad_norm": 6.152993679046631,
      "learning_rate": 4.5085910652920964e-05,
      "loss": 1.7856,
      "step": 800
    },
    {
      "epoch": 1.670103092783505,
      "grad_norm": 6.868847846984863,
      "learning_rate": 4.4398625429553264e-05,
      "loss": 1.8322,
      "step": 810
    },
    {
      "epoch": 1.690721649484536,
      "grad_norm": 7.361021995544434,
      "learning_rate": 4.371134020618557e-05,
      "loss": 1.9079,
      "step": 820
    },
    {
      "epoch": 1.7113402061855671,
      "grad_norm": 8.363999366760254,
      "learning_rate": 4.302405498281787e-05,
      "loss": 1.7888,
      "step": 830
    },
    {
      "epoch": 1.731958762886598,
      "grad_norm": 10.140145301818848,
      "learning_rate": 4.2336769759450174e-05,
      "loss": 1.9191,
      "step": 840
    },
    {
      "epoch": 1.7525773195876289,
      "grad_norm": 5.7063984870910645,
      "learning_rate": 4.164948453608248e-05,
      "loss": 1.7722,
      "step": 850
    },
    {
      "epoch": 1.7731958762886597,
      "grad_norm": 9.874116897583008,
      "learning_rate": 4.0962199312714775e-05,
      "loss": 2.0905,
      "step": 860
    },
    {
      "epoch": 1.7938144329896906,
      "grad_norm": 9.074738502502441,
      "learning_rate": 4.027491408934708e-05,
      "loss": 1.696,
      "step": 870
    },
    {
      "epoch": 1.8144329896907216,
      "grad_norm": 9.422457695007324,
      "learning_rate": 3.9587628865979384e-05,
      "loss": 1.6233,
      "step": 880
    },
    {
      "epoch": 1.8350515463917527,
      "grad_norm": 5.989528656005859,
      "learning_rate": 3.8900343642611685e-05,
      "loss": 1.6762,
      "step": 890
    },
    {
      "epoch": 1.8556701030927836,
      "grad_norm": 9.668604850769043,
      "learning_rate": 3.8213058419243985e-05,
      "loss": 1.8124,
      "step": 900
    },
    {
      "epoch": 1.8762886597938144,
      "grad_norm": 7.839090824127197,
      "learning_rate": 3.752577319587629e-05,
      "loss": 1.7244,
      "step": 910
    },
    {
      "epoch": 1.8969072164948453,
      "grad_norm": 6.348371505737305,
      "learning_rate": 3.6838487972508594e-05,
      "loss": 1.6572,
      "step": 920
    },
    {
      "epoch": 1.9175257731958761,
      "grad_norm": 8.772089958190918,
      "learning_rate": 3.6151202749140895e-05,
      "loss": 1.6003,
      "step": 930
    },
    {
      "epoch": 1.9381443298969072,
      "grad_norm": 5.8344831466674805,
      "learning_rate": 3.54639175257732e-05,
      "loss": 1.7814,
      "step": 940
    },
    {
      "epoch": 1.9587628865979383,
      "grad_norm": 11.152522087097168,
      "learning_rate": 3.4776632302405496e-05,
      "loss": 2.0183,
      "step": 950
    },
    {
      "epoch": 1.9793814432989691,
      "grad_norm": 6.933783531188965,
      "learning_rate": 3.4089347079037804e-05,
      "loss": 1.7911,
      "step": 960
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.334726810455322,
      "learning_rate": 3.3402061855670105e-05,
      "loss": 1.9188,
      "step": 970
    }
  ],
  "logging_steps": 10,
  "max_steps": 1455,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4252798769995776.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
